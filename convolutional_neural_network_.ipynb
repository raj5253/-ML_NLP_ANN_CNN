{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3DR-eO17geWu"},"source":["# Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"sCV30xyVhFbE"},"outputs":[],"source":["# training set has 4000 images of cats and 4000 images of dogs\n","# test set has 1000 images for cats and 1000 for dogs\n","import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'2.10.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oxQxCBWyoGPE"},"source":["## Part 1 - Data Preprocessing"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MvE-heJNo3GG"},"source":["### Preprocessing the Training set"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8000 files belonging to 2 classes.\n"]}],"source":["# overfitting of model should be avoided in CNN.\n","#  overtting : training set gives 95 % accuracy, but test set gives 5% accuracy\n","\n","\n","training_set = tf.keras.utils.image_dataset_from_directory(\n","    './dataset/training_set/',\n","    labels=\"inferred\",\n","    label_mode=\"binary\", #binary means there can be only two classes -> 0 and 1\n","    class_names=None,\n","    color_mode=\"rgb\",\n","    batch_size=32,\n","    image_size=(64, 64),\n","    shuffle=True,\n","    seed=123, # optional random seed for shuffing of information\n","    validation_split=None,\n","    subset=None,\n","    interpolation=\"bilinear\",\n","    follow_links=False,\n","    crop_to_aspect_ratio=False,\n",")\n","# images is a metric of shpae : -......... https://keras.io/api/data_loading/image/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2000 files belonging to 2 classes.\n"]}],"source":["test_set = tf.keras.utils.image_dataset_from_directory(\n","    './dataset/test_set/',\n","    labels=\"inferred\",\n","    label_mode=\"binary\",\n","    class_names=None,\n","    color_mode=\"rgb\",\n","    batch_size=32,\n","    image_size=(64, 64),\n","    shuffle=True,\n","    seed=123, # None\n","    validation_split=None,\n","    subset=None,\n","    interpolation=\"bilinear\",\n","    follow_links=False,\n","    crop_to_aspect_ratio=False,\n",")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"af8O4l90gk7B"},"source":["## Part 2 - Building the CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["cnn = tf.keras.models.Sequential()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u5YJj_XMl5LF"},"source":["### Step 1 - Convolution"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3 , activation= 'relu', input_shape = [64, 64, 3])) # 3 rfor 3 layes of rbg"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tf87FpvxmNOJ"},"source":["### Step 2 - Pooling"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.MaxPool2D( pool_size=2, strides=2))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3 , activation= 'relu'))\n","cnn.add(tf.keras.layers.MaxPool2D( pool_size=2, strides=2))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.Flatten())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))  # you need only 1 node in ouput layer.The node will have 0 for cats, 1 for dofgs\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D6XkI90snSDl"},"source":["## Part 3 - Training the CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["cnn.compile(optimizer='adam', loss= 'binary_crossentropy', metrics= ['accuracy'])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.6589 - accuracy: 0.5991 - val_loss: 0.6808 - val_accuracy: 0.5645\n","Epoch 2/25\n","250/250 [==============================] - 16s 65ms/step - loss: 0.5832 - accuracy: 0.6817 - val_loss: 0.6878 - val_accuracy: 0.5865\n","Epoch 3/25\n","250/250 [==============================] - 19s 75ms/step - loss: 0.4877 - accuracy: 0.7516 - val_loss: 0.7639 - val_accuracy: 0.5895\n","Epoch 4/25\n","250/250 [==============================] - 28s 112ms/step - loss: 0.3694 - accuracy: 0.8232 - val_loss: 0.8809 - val_accuracy: 0.6060\n","Epoch 5/25\n","250/250 [==============================] - 16s 63ms/step - loss: 0.2795 - accuracy: 0.8767 - val_loss: 1.0041 - val_accuracy: 0.6175\n","Epoch 6/25\n","250/250 [==============================] - 26s 103ms/step - loss: 0.2049 - accuracy: 0.9166 - val_loss: 1.1921 - val_accuracy: 0.6040\n","Epoch 7/25\n","250/250 [==============================] - 48s 191ms/step - loss: 0.1829 - accuracy: 0.9291 - val_loss: 1.2930 - val_accuracy: 0.5965\n","Epoch 8/25\n","250/250 [==============================] - 12s 46ms/step - loss: 0.1394 - accuracy: 0.9486 - val_loss: 1.5116 - val_accuracy: 0.5940\n","Epoch 9/25\n","250/250 [==============================] - 32s 129ms/step - loss: 0.1608 - accuracy: 0.9396 - val_loss: 1.3396 - val_accuracy: 0.6080\n","Epoch 10/25\n","250/250 [==============================] - 20s 80ms/step - loss: 0.1424 - accuracy: 0.9506 - val_loss: 1.4632 - val_accuracy: 0.5980\n","Epoch 11/25\n","250/250 [==============================] - 16s 63ms/step - loss: 0.1028 - accuracy: 0.9636 - val_loss: 1.6969 - val_accuracy: 0.6020\n","Epoch 12/25\n","250/250 [==============================] - 21s 84ms/step - loss: 0.0821 - accuracy: 0.9722 - val_loss: 2.1500 - val_accuracy: 0.6070\n","Epoch 13/25\n","250/250 [==============================] - 36s 143ms/step - loss: 0.0654 - accuracy: 0.9786 - val_loss: 1.9680 - val_accuracy: 0.6055\n","Epoch 14/25\n","250/250 [==============================] - 20s 78ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 2.0366 - val_accuracy: 0.6000\n","Epoch 15/25\n","250/250 [==============================] - 23s 90ms/step - loss: 0.0806 - accuracy: 0.9714 - val_loss: 2.1917 - val_accuracy: 0.6095\n","Epoch 16/25\n","250/250 [==============================] - 19s 74ms/step - loss: 0.0803 - accuracy: 0.9730 - val_loss: 2.2099 - val_accuracy: 0.5950\n","Epoch 17/25\n","250/250 [==============================] - 38s 152ms/step - loss: 0.0644 - accuracy: 0.9790 - val_loss: 2.2436 - val_accuracy: 0.6070\n","Epoch 18/25\n","250/250 [==============================] - 17s 69ms/step - loss: 0.0838 - accuracy: 0.9753 - val_loss: 2.5918 - val_accuracy: 0.5990\n","Epoch 19/25\n","250/250 [==============================] - 37s 146ms/step - loss: 0.0635 - accuracy: 0.9800 - val_loss: 2.6070 - val_accuracy: 0.6025\n","Epoch 20/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.0605 - accuracy: 0.9821 - val_loss: 2.5079 - val_accuracy: 0.5995\n","Epoch 21/25\n","250/250 [==============================] - 46s 186ms/step - loss: 0.0579 - accuracy: 0.9805 - val_loss: 2.6650 - val_accuracy: 0.6055\n","Epoch 22/25\n","250/250 [==============================] - 19s 73ms/step - loss: 0.0449 - accuracy: 0.9881 - val_loss: 2.7246 - val_accuracy: 0.6105\n","Epoch 23/25\n","250/250 [==============================] - 14s 56ms/step - loss: 0.0499 - accuracy: 0.9844 - val_loss: 2.6760 - val_accuracy: 0.6055\n","Epoch 24/25\n","250/250 [==============================] - 14s 57ms/step - loss: 0.0692 - accuracy: 0.9799 - val_loss: 2.7561 - val_accuracy: 0.5990\n","Epoch 25/25\n","250/250 [==============================] - 15s 60ms/step - loss: 0.0341 - accuracy: 0.9893 - val_loss: 2.9735 - val_accuracy: 0.6175\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1dc9b798730>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["cnn.fit(x=training_set,validation_data= test_set, epochs=25)"]},{"attachments":{},"cell_type":"markdown","metadata":{"colab_type":"text","id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 30ms/step\n","[[0.01796885]]\n","cat\n"]}],"source":["import numpy as np\n","\n","test_image = tf.keras.utils.load_img(\n","    'dataset/single_prediction/cat_or_dog2.jpg',\n","    grayscale=False,\n","    color_mode=\"rgb\",\n","    target_size=(64, 64),\n","    interpolation=\"bilinear\", #nearest\n","    keep_aspect_ratio=False,\n",")\n","\n","test_image = tf.keras.utils.img_to_array(test_image)\n","\n","test_image = np.array([test_image])  # Convert single image to a batch. or by \n","# test_image = np.expand_dims(test_image, axis=0) # use only any one of these to recude dimension\n","\n","result = cnn.predict(test_image)\n","# training_set.class_indicies\n","print(result)\n","if round(result[0][0]) == 1 :\n","    prediction = 'dog'\n","else : prediction ='cat'\n","print(prediction)\n","\n","############ PIL (python image library)  was not installed, so error was generated. So, i installed Pillow(fork friend of PIL)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyON0YxX/oky4tPbqCLnFjWD","collapsed_sections":[],"name":"convolutional_neural_network.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
